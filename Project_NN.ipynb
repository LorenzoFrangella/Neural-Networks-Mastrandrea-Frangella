{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Libraries required for developing the project"
      ],
      "metadata": {
        "id": "smx0l5hho5NF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install laion-clap\n",
        "!pip install transformers==4.30.2\n",
        "!pip install soundfile\n",
        "!pip install librosa\n",
        "!pip install torchlibrosa\n",
        "!pip install ftfy\n",
        "!pip install braceexpand\n",
        "!pip install webdataset\n",
        "!pip install wget\n",
        "!pip install wandb\n",
        "!pip install llvmlite\n",
        "!pip install scipy\n",
        "!pip install scikit-learn\n",
        "!pip install pandas\n",
        "!pip install h5py\n",
        "!pip install tqdm\n",
        "!pip install regex\n",
        "!pip install torch\n",
        "!pip install pytube\n",
        "!pip install pydub"
      ],
      "metadata": {
        "id": "doc6O-mjOY2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBKZFc14TIfh",
        "outputId": "e81477d2-8484-483c-e203-0c0557c58729"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load the specified checkpoint /root/.cache/huggingface/hub/models--lukewys--laion_clap/snapshots/b3708341862f581175dba5c356a4ebf74a9b6651/music_speech_audioset_epoch_15_esc_89.98.pt from users.\n",
            "Load Checkpoint...\n",
            "logit_scale_a \t Loaded\n",
            "logit_scale_t \t Loaded\n",
            "audio_branch.spectrogram_extractor.stft.conv_real.weight \t Loaded\n",
            "audio_branch.spectrogram_extractor.stft.conv_imag.weight \t Loaded\n",
            "audio_branch.logmel_extractor.melW \t Loaded\n",
            "audio_branch.bn0.weight \t Loaded\n",
            "audio_branch.bn0.bias \t Loaded\n",
            "audio_branch.patch_embed.proj.weight \t Loaded\n",
            "audio_branch.patch_embed.proj.bias \t Loaded\n",
            "audio_branch.patch_embed.norm.weight \t Loaded\n",
            "audio_branch.patch_embed.norm.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.0.norm1.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.0.norm1.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.0.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.0.blocks.0.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.0.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.0.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.0.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.0.norm2.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.0.norm2.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.0.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.0.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.0.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.0.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.1.norm1.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.1.norm1.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.1.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.0.blocks.1.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.1.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.1.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.1.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.1.norm2.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.1.norm2.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.1.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.1.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.1.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.1.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.0.downsample.reduction.weight \t Loaded\n",
            "audio_branch.layers.0.downsample.norm.weight \t Loaded\n",
            "audio_branch.layers.0.downsample.norm.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.0.norm1.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.0.norm1.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.0.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.1.blocks.0.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.0.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.0.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.0.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.0.norm2.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.0.norm2.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.0.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.0.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.0.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.0.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.1.norm1.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.1.norm1.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.1.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.1.blocks.1.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.1.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.1.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.1.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.1.norm2.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.1.norm2.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.1.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.1.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.1.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.1.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.1.downsample.reduction.weight \t Loaded\n",
            "audio_branch.layers.1.downsample.norm.weight \t Loaded\n",
            "audio_branch.layers.1.downsample.norm.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.0.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.0.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.0.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.0.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.0.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.0.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.0.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.0.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.0.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.0.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.0.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.0.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.0.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.1.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.1.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.1.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.1.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.1.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.1.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.1.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.1.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.1.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.1.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.1.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.1.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.1.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.2.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.2.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.2.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.2.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.2.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.2.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.2.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.2.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.2.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.2.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.2.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.2.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.2.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.3.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.3.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.3.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.3.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.3.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.3.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.3.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.3.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.3.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.3.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.3.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.3.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.3.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.4.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.4.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.4.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.4.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.4.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.4.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.4.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.4.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.4.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.4.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.4.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.4.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.4.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.5.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.5.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.5.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.5.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.5.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.5.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.5.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.5.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.5.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.5.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.5.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.5.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.5.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.6.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.6.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.6.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.6.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.6.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.6.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.6.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.6.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.6.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.6.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.6.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.6.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.6.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.7.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.7.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.7.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.7.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.7.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.7.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.7.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.7.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.7.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.7.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.7.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.7.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.7.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.8.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.8.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.8.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.8.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.8.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.8.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.8.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.8.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.8.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.8.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.8.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.8.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.8.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.9.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.9.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.9.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.9.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.9.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.9.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.9.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.9.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.9.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.9.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.9.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.9.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.9.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.10.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.10.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.10.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.10.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.10.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.10.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.10.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.10.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.10.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.10.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.10.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.10.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.10.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.11.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.11.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.11.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.11.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.11.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.11.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.11.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.11.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.11.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.11.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.11.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.11.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.11.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.downsample.reduction.weight \t Loaded\n",
            "audio_branch.layers.2.downsample.norm.weight \t Loaded\n",
            "audio_branch.layers.2.downsample.norm.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.0.norm1.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.0.norm1.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.0.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.3.blocks.0.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.0.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.0.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.0.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.0.norm2.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.0.norm2.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.0.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.0.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.0.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.0.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.1.norm1.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.1.norm1.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.1.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.3.blocks.1.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.1.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.1.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.1.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.1.norm2.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.1.norm2.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.1.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.1.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.1.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.1.mlp.fc2.bias \t Loaded\n",
            "audio_branch.norm.weight \t Loaded\n",
            "audio_branch.norm.bias \t Loaded\n",
            "audio_branch.tscam_conv.weight \t Loaded\n",
            "audio_branch.tscam_conv.bias \t Loaded\n",
            "audio_branch.head.weight \t Loaded\n",
            "audio_branch.head.bias \t Loaded\n",
            "text_branch.embeddings.word_embeddings.weight \t Loaded\n",
            "text_branch.embeddings.position_embeddings.weight \t Loaded\n",
            "text_branch.embeddings.token_type_embeddings.weight \t Loaded\n",
            "text_branch.embeddings.LayerNorm.weight \t Loaded\n",
            "text_branch.embeddings.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.0.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.0.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.0.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.0.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.0.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.0.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.0.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.0.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.0.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.0.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.0.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.0.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.0.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.0.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.0.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.0.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.1.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.1.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.1.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.1.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.1.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.1.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.1.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.1.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.1.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.1.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.1.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.1.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.1.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.1.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.1.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.1.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.2.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.2.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.2.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.2.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.2.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.2.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.2.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.2.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.2.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.2.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.2.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.2.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.2.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.2.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.2.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.2.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.3.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.3.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.3.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.3.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.3.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.3.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.3.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.3.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.3.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.3.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.3.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.3.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.3.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.3.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.3.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.3.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.4.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.4.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.4.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.4.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.4.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.4.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.4.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.4.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.4.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.4.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.4.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.4.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.4.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.4.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.4.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.4.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.5.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.5.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.5.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.5.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.5.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.5.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.5.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.5.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.5.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.5.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.5.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.5.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.5.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.5.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.5.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.5.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.6.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.6.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.6.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.6.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.6.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.6.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.6.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.6.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.6.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.6.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.6.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.6.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.6.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.6.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.6.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.6.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.7.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.7.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.7.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.7.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.7.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.7.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.7.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.7.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.7.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.7.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.7.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.7.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.7.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.7.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.7.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.7.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.8.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.8.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.8.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.8.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.8.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.8.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.8.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.8.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.8.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.8.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.8.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.8.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.8.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.8.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.8.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.8.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.9.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.9.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.9.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.9.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.9.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.9.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.9.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.9.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.9.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.9.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.9.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.9.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.9.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.9.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.9.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.9.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.10.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.10.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.10.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.10.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.10.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.10.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.10.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.10.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.10.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.10.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.10.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.10.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.10.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.10.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.10.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.10.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.11.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.11.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.11.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.11.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.11.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.11.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.11.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.11.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.11.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.11.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.11.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.11.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.11.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.11.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.11.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.11.output.LayerNorm.bias \t Loaded\n",
            "text_branch.pooler.dense.weight \t Loaded\n",
            "text_branch.pooler.dense.bias \t Loaded\n",
            "text_transform.sequential.0.weight \t Loaded\n",
            "text_transform.sequential.0.bias \t Loaded\n",
            "text_transform.sequential.3.weight \t Loaded\n",
            "text_transform.sequential.3.bias \t Loaded\n",
            "text_projection.0.weight \t Loaded\n",
            "text_projection.0.bias \t Loaded\n",
            "text_projection.2.weight \t Loaded\n",
            "text_projection.2.bias \t Loaded\n",
            "audio_transform.sequential.0.weight \t Loaded\n",
            "audio_transform.sequential.0.bias \t Loaded\n",
            "audio_transform.sequential.3.weight \t Loaded\n",
            "audio_transform.sequential.3.bias \t Loaded\n",
            "audio_projection.0.weight \t Loaded\n",
            "audio_projection.0.bias \t Loaded\n",
            "audio_projection.2.weight \t Loaded\n",
            "audio_projection.2.bias \t Loaded\n"
          ]
        }
      ],
      "source": [
        "import laion_clap\n",
        "import torch\n",
        "from huggingface_hub import hf_hub_download\n",
        "import librosa\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "model = laion_clap.CLAP_Module(enable_fusion=False, amodel= 'HTSAT-base').to(device='cuda')\n",
        "dataset_path = hf_hub_download(repo_id=\"lukewys/laion_clap\", filename=\"music_speech_audioset_epoch_15_esc_89.98.pt\")\n",
        "model.load_ckpt(dataset_path)\n",
        "# quantization\n",
        "def int16_to_float32(x):\n",
        "    return (x / 32767.0).astype(np.float32)\n",
        "\n",
        "\n",
        "def float32_to_int16(x):\n",
        "    x = np.clip(x, a_min=-1., a_max=1.)\n",
        "    return (x * 32767.).astype(np.int16)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pat = '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'\n",
        "!git clone https://{pat}@github.com/LorenzoFrangella/Neural-Networks-Mastrandrea-Frangella"
      ],
      "metadata": {
        "id": "uP76X0i90iPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv Neural-Networks-Mastrandrea-Frangella/download .\n",
        "!mv Neural-Networks-Mastrandrea-Frangella/tmp ."
      ],
      "metadata": {
        "id": "YvnAL2ZJ-32r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audios_path = ['./download/--PJHxphWEs.mp3','./download/--aE2O5G5WE.mp3']\n",
        "caso = torch.rand(10,2,16000)\n",
        "audio_embed = model.get_audio_embedding_from_filelist(audios_path,use_tensor=True)\n",
        "print(audio_embed.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EREKC-qXYB5",
        "outputId": "abdbddaf-611f-425b-ed16-6ddc5a1cf848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "audio_data, _ = torchaudio.load('./download/--aE2O5G5WE.mp3')\n",
        "print(audio_data.shape)\n",
        "audio_data = audio_data.reshape(1, -1) # Make it (1,T) or (N,T)\n",
        "print(audio_data.shape)\n",
        "audio_embed = model.get_audio_embedding_from_data(audio_data,use_tensor=True)\n",
        "print(audio_embed.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wr7EID-EYotI",
        "outputId": "6e6c7197-e576-4735-d23d-25d4fa98a03c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 320000])\n",
            "torch.Size([1, 640000])\n",
            "torch.Size([1, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from  pytube import YouTube\n",
        "import os\n",
        "from pydub import AudioSegment\n",
        "import csv\n",
        "import random\n",
        "import math\n",
        "import torch\n",
        "import torchaudio\n",
        "\n",
        "def cut_audio(input_file, output_file, start_time, end_time):\n",
        "    audio = AudioSegment.from_file(input_file)\n",
        "    audio = audio.set_frame_rate(32000)\n",
        "    cut_audio = audio[start_time:end_time]\n",
        "    cut_audio.export(output_file, format=\"mp3\")\n",
        "\n",
        "def get_mixture_audio(audio1,audio2):\n",
        "\n",
        "    E1 = torch.square(torch.norm(audio1,p=2))\n",
        "    E2 = torch.square(torch.norm(audio2,p=2))\n",
        "\n",
        "    alpha = torch.sqrt(E1/E2)\n",
        "\n",
        "    x = audio1 + alpha * audio2\n",
        "    return x\n",
        "\n",
        "def get_audio_clip(video_id, start, end, download=True):\n",
        "\n",
        "    if download:\n",
        "\n",
        "        if f\"{video_id}.mp3\" not in os.listdir(\"./download\"):\n",
        "\n",
        "            video_url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
        "            selected_video = YouTube(video_url)\n",
        "            audio = selected_video.streams.filter(only_audio = True).first()\n",
        "            path_dest = audio.download(\"./download\", filename=f\"{video_id}.mp3\")\n",
        "            cut_audio(path_dest, path_dest, start*1000, end*1000)\n",
        "\n",
        "        path_dest = f\"./download/{video_id}.mp3\"\n",
        "\n",
        "    else:\n",
        "\n",
        "        if f\"{video_id}.mp3\" not in os.listdir(\"./download\"):\n",
        "            return \"\"\n",
        "\n",
        "        else:\n",
        "            path_dest = f\"./download/{video_id}.mp3\"\n",
        "\n",
        "    return path_dest\n",
        "\n",
        "def download_all_dataset():\n",
        "    with open(\"./Neural-Networks-Mastrandrea-Frangella/mastrandrea.csv\", mode ='r')as file:\n",
        "        csvFile = csv.reader(file)\n",
        "        for lines in csvFile:\n",
        "            video_id = lines[0]\n",
        "            start = lines[1]\n",
        "            end = lines[2]\n",
        "            try:\n",
        "                get_audio_clip(video_id,float(start),float(end))\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "def get_random_files(directory, count=20):\n",
        "    files = os.listdir(directory)\n",
        "    random_files = random.sample(files, count)\n",
        "    return random_files\n",
        "\n",
        "def get_batch(batch_size,dataset_path,labels_file,modality):\n",
        "    batch_size=batch_size*2\n",
        "    random_samples = get_random_files(dataset_path,batch_size)\n",
        "    half = int(batch_size/2)\n",
        "\n",
        "    first_subset = random_samples[0:half]\n",
        "    second_subset = random_samples[half:batch_size]\n",
        "\n",
        "    labels_dict = {}\n",
        "\n",
        "    with open(labels_file, mode ='r')as file:\n",
        "        csvFile = csv.reader(file)\n",
        "        for lines in csvFile:\n",
        "            label = lines[4][1:-1]\n",
        "            label = label.replace(\"[\",\"\")\n",
        "            label = label.replace(\"]\",\"\")\n",
        "            label = label.replace(\",\",\"\")\n",
        "            label = label.replace(\"'\",\"\")\n",
        "            labels_dict[lines[0]]=[label]\n",
        "\n",
        "    batch = []\n",
        "    labels = []\n",
        "    target = []\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(half):\n",
        "        audio1,sample_rate1 = torchaudio.load(f\"./download/{first_subset[i]}\")\n",
        "        audio2,sample_rate2 = torchaudio.load(f\"./download/{second_subset[i]}\")\n",
        "\n",
        "\n",
        "\n",
        "        # computing starting and ending frame for audio1\n",
        "        start1 = random.randint(0,160000)\n",
        "        end1 = start1 + 160000\n",
        "        # computing starting and ending frame for audio2\n",
        "        start2 = random.randint(0,160000)\n",
        "        end2 = start2 + 160000\n",
        "\n",
        "        audio1 = audio1[:,start1:end1]\n",
        "        audio2 = audio2[:,start2:end2]\n",
        "\n",
        "        target.append(audio1)\n",
        "\n",
        "        mixed = get_mixture_audio(audio1,audio2)\n",
        "\n",
        "        batch.append(mixed)\n",
        "        labels.append(labels_dict[first_subset[i][:-4]][0])\n",
        "\n",
        "    batch = torch.stack(batch,dim=0)\n",
        "    target = torch.stack(target,dim=0)\n",
        "\n",
        "    if modality==\"text\":\n",
        "        return (batch,target,labels,False)\n",
        "\n",
        "    else:\n",
        "        if random.uniform(0,1) > 0.5:\n",
        "            labels = [\"./download/\"+elem for elem in first_subset]\n",
        "            return (batch,target,labels,True)\n",
        "\n",
        "    return (batch,target,labels,False)"
      ],
      "metadata": {
        "id": "Yg-1a7G2TlDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch,target,labels,isAudio = get_batch(5,\"./download\",\"./Neural-Networks-Mastrandrea-Frangella/new_balanced.csv\",\"text\")\n"
      ],
      "metadata": {
        "id": "1EuhSAUtGs-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(batch.shape)\n",
        "print(target.shape)\n",
        "print(len(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1g_zgHMw9KGu",
        "outputId": "5f3055ad-e6db-4a79-d7e9-8be20998be53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 2, 160000])\n",
            "torch.Size([5, 2, 160000])\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm ./download/.DS_Store"
      ],
      "metadata": {
        "id": "6e1x0Yui9Alv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f299c67-811c-47f3-c732-c3346257c5be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove './download/.DS_Store': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r ./download.zip /content/download"
      ],
      "metadata": {
        "id": "RTC53GDSTKve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install numpy\n",
        "!pip install torchlibrosa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUbqIupp9XT8",
        "outputId": "e20848a9-3369-4c41-b9e1-73ce89f68e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: torchlibrosa in /usr/local/lib/python3.10/dist-packages (0.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchlibrosa) (1.23.5)\n",
            "Requirement already satisfied: librosa>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchlibrosa) (0.10.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.8.0->torchlibrosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.8.0->torchlibrosa) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.8.0->torchlibrosa) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.8.0->torchlibrosa) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.8.0->torchlibrosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.8.0->torchlibrosa) (0.58.1)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.8.0->torchlibrosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.8.0->torchlibrosa) (1.8.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.8.0->torchlibrosa) (0.3.7)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.8.0->torchlibrosa) (4.5.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.8.0->torchlibrosa) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.8.0->torchlibrosa) (1.0.7)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa>=0.8.0->torchlibrosa) (0.41.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa>=0.8.0->torchlibrosa) (4.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa>=0.8.0->torchlibrosa) (23.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa>=0.8.0->torchlibrosa) (2.31.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa>=0.8.0->torchlibrosa) (3.2.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa>=0.8.0->torchlibrosa) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa>=0.8.0->torchlibrosa) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.8.0->torchlibrosa) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.8.0->torchlibrosa) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.8.0->torchlibrosa) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.8.0->torchlibrosa) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchlibrosa.stft import STFT, ISTFT, magphase\n",
        "import numpy as np\n",
        "\n",
        "stft = STFT(n_fft=1024,\n",
        "            hop_length=320,\n",
        "            win_length=1024,\n",
        "            window='hann',\n",
        "            center=True,\n",
        "            pad_mode='reflect',\n",
        "            freeze_parameters=True).to(device='cuda')\n",
        "\n",
        "istft = ISTFT(\n",
        "            n_fft=1024,\n",
        "            hop_length=320,\n",
        "            win_length=1024,\n",
        "            window='hann',\n",
        "            center=True,\n",
        "            pad_mode='reflect',\n",
        "            freeze_parameters=True\n",
        "        ).to(device='cuda')\n",
        "\n",
        "def init_bn(bn):\n",
        "    bn.bias.data.fill_(0.0)\n",
        "    bn.weight.data.fill_(1.0)\n",
        "\n",
        "def init_layer(layer):\n",
        "    nn.init.xavier_uniform_(layer.weight)\n",
        "\n",
        "    if hasattr(layer, \"bias\"):\n",
        "        if layer.bias is not None:\n",
        "            layer.bias.data.fill_(0.0)\n",
        "\n",
        "def from_audio_to_spectogram(audios):\n",
        "    magnitudes = []\n",
        "    cosines = []\n",
        "    sines = []\n",
        "\n",
        "    for i in range(audios.shape[1]):\n",
        "\n",
        "        (real,img) = stft(audios[:,i,:])\n",
        "        mag = torch.clamp(real ** 2 + img ** 2, 1e-10, np.inf) ** 0.5\n",
        "        cos = real / mag\n",
        "        sin = img / mag\n",
        "        magnitudes.append(real)\n",
        "        cosines.append(cos)\n",
        "        sines.append(sin)\n",
        "    mags = torch.cat(magnitudes, dim=1)\n",
        "    coss = torch.cat(cosines, dim=1)\n",
        "    sins = torch.cat(sines, dim=1)\n",
        "\n",
        "    return mags,coss,sins"
      ],
      "metadata": {
        "id": "BulIBIoQ82uN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FilmModule(nn.Module):\n",
        "    def __init__(self,input_size,output_size):\n",
        "        super(FilmModule, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(input_size, output_size * 2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(output_size * 2, output_size),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        #init_layer(self.linear)\n",
        "\n",
        "    def forward(self,data,embedding_vector):\n",
        "        x = self.linear(embedding_vector)\n",
        "        x = data + x[...,None,None]\n",
        "\n",
        "        return x\n",
        "\n",
        "#film = FilmModule(512,32)\n",
        "#random_embedding = torch.rand(4,512)\n",
        "#random_data = torch.rand(4,32,512,512)\n",
        "#print(film(random_data,random_embedding).shape)"
      ],
      "metadata": {
        "id": "YVcN0sWHqJ6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self,input_channels, output_channels, embedding_size, momentum,downsample):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.downsample = downsample\n",
        "        self.Film1 = FilmModule(embedding_size,input_channels)\n",
        "        self.Film2 = FilmModule(embedding_size,output_channels)\n",
        "\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(input_channels,momentum=momentum)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=input_channels,\n",
        "            out_channels=output_channels,\n",
        "            kernel_size=(3,3),\n",
        "            stride=(1,1),\n",
        "            dilation=(1,1),\n",
        "            padding=(1,1),\n",
        "            bias=False\n",
        "            )\n",
        "\n",
        "        self.bn2 = nn.BatchNorm2d(output_channels,momentum=momentum)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels=output_channels,\n",
        "            out_channels=output_channels,\n",
        "            kernel_size=(3,3),\n",
        "            stride=(1,1),\n",
        "            dilation=(1,1),\n",
        "            padding=(1,1),\n",
        "            bias=False\n",
        "        )\n",
        "\n",
        "        if input_channels != output_channels:\n",
        "            self.residual_convolution = nn.Conv2d(\n",
        "                in_channels=input_channels,\n",
        "                out_channels=output_channels,\n",
        "                kernel_size=(1,1),\n",
        "                stride=(1,1),\n",
        "                padding=(0,0),\n",
        "            )\n",
        "            self.has_residual_connection = True\n",
        "        else:\n",
        "            self.has_residual_connection = False\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "\n",
        "    def init_weights(self):\n",
        "        init_bn(self.bn1)\n",
        "        init_bn(self.bn2)\n",
        "        init_layer(self.conv1)\n",
        "        init_layer(self.conv2)\n",
        "\n",
        "        if self.has_residual_connection:\n",
        "            init_layer(self.residual_convolution)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,input_tensor,embedding_vector):\n",
        "\n",
        "        x = self.bn1(input_tensor)\n",
        "        x = self.Film1(x,embedding_vector)\n",
        "        x = F.leaky_relu(x,negative_slope=0.01)\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.Film2(x,embedding_vector)\n",
        "        x = F.leaky_relu(x,negative_slope=0.01)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        if self.has_residual_connection:\n",
        "            y = self.residual_convolution(input_tensor)\n",
        "            x = x + y\n",
        "\n",
        "        x_pool = F.avg_pool2d(x,self.downsample)\n",
        "\n",
        "        return x, x_pool"
      ],
      "metadata": {
        "id": "hTIYXEKPNmVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self,input_size, output_size,embedding_size,momentum,upsample):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.upsample = upsample\n",
        "\n",
        "        self.conv1 = torch.nn.ConvTranspose2d(\n",
        "            in_channels=input_size,\n",
        "            out_channels=output_size,\n",
        "            kernel_size=self.upsample,\n",
        "            stride=self.upsample,\n",
        "            padding=(0,0),\n",
        "            bias=False,\n",
        "            dilation=(1,1)\n",
        "\n",
        "        )\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(input_size,momentum=momentum)\n",
        "\n",
        "        #self.conv_block2 = ConvBlockRes(\n",
        "        #    out_channels * 2, out_channels, kernel_size, momentum, has_film,\n",
        "\n",
        "        self.Film1 = FilmModule(embedding_size,input_size)\n",
        "        self.Film2 = FilmModule(embedding_size,output_size*2)\n",
        "        self.Film3 = FilmModule(embedding_size,output_size)\n",
        "\n",
        "        self.bn2 = nn.BatchNorm2d(output_size*2,momentum=momentum)\n",
        "        self.bn3 = nn.BatchNorm2d(output_size,momentum=momentum)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels=output_size*2,\n",
        "            out_channels=output_size,\n",
        "            kernel_size=(3,3),\n",
        "            stride=(1,1),\n",
        "            dilation=(1,1),\n",
        "            padding=(1,1),\n",
        "            bias=False\n",
        "        )\n",
        "\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            in_channels=output_size,\n",
        "            out_channels=output_size,\n",
        "            kernel_size=(3,3),\n",
        "            stride=(1,1),\n",
        "            dilation=(1,1),\n",
        "            padding=(1,1),\n",
        "            bias=False\n",
        "        )\n",
        "\n",
        "        self.residual_convolution = nn.Conv2d(\n",
        "                in_channels=output_size*2,\n",
        "                out_channels=output_size,\n",
        "                kernel_size=(1,1),\n",
        "                stride=(1,1),\n",
        "                padding=(0,0),\n",
        "            )\n",
        "        self.has_residual_connection = True\n",
        "\n",
        "        self.bn4 = nn.BatchNorm2d(input_size,momentum=momentum)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        init_bn(self.bn1)\n",
        "        init_bn(self.bn2)\n",
        "        init_bn(self.bn3)\n",
        "\n",
        "        init_layer(self.conv1)\n",
        "        init_layer(self.conv2)\n",
        "        init_layer(self.conv3)\n",
        "\n",
        "        if self.has_residual_connection:\n",
        "            init_layer(self.residual_convolution)\n",
        "\n",
        "    def forward(self,input_tensor,concat_tensor,embedding_vector):\n",
        "        x = self.bn1(input_tensor)\n",
        "        x = self.Film1(x,embedding_vector)\n",
        "        x = F.leaky_relu(x)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        x = torch.cat((x,concat_tensor), dim=1)\n",
        "        x_res = x\n",
        "        x = self.bn2(x)\n",
        "        x = self.Film2(x,embedding_vector)\n",
        "        x = F.leaky_relu(x,negative_slope=0.01)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.Film3(x,embedding_vector)\n",
        "        x = F.leaky_relu(x,negative_slope=0.01)\n",
        "        x = self.conv3(x)\n",
        "        if self.has_residual_connection:\n",
        "            y = self.residual_convolution(x_res)\n",
        "\n",
        "\n",
        "\n",
        "            x = x + y\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "BZxNy1fDNnuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResUnet(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(ResUnet, self).__init__()\n",
        "\n",
        "        self.input_size = input_size;\n",
        "        self.output_size = output_size;\n",
        "\n",
        "        self.momentum = 0.01\n",
        "\n",
        "\n",
        "        # instanziare la preconv che è una conv2d\n",
        "\n",
        "        # definire la classe degli encoder block\n",
        "        # definire la classe dei decoder block\n",
        "\n",
        "        self.batch_norm0 = nn.BatchNorm2d(513,momentum=self.momentum)\n",
        "        self.loss = nn.L1Loss()\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
        "        self.preconvolution = nn.Conv2d(\n",
        "            in_channels=input_size,\n",
        "            out_channels=32,\n",
        "            kernel_size=(1,1),\n",
        "            stride=(1,1),\n",
        "            padding=(0,0),\n",
        "            bias=True\n",
        "        )\n",
        "\n",
        "        self.EncoderBlock1 = EncoderBlock(\n",
        "            input_channels=32,\n",
        "            output_channels=32,\n",
        "            downsample=(2,2),\n",
        "            embedding_size=512,\n",
        "            momentum=0.01\n",
        "        )\n",
        "\n",
        "        self.EncoderBlock2 = EncoderBlock(\n",
        "            input_channels=32,\n",
        "            output_channels=64,\n",
        "            downsample=(2,2),\n",
        "            embedding_size=512,\n",
        "            momentum=0.01\n",
        "        )\n",
        "\n",
        "        self.EncoderBlock3 = EncoderBlock(\n",
        "            input_channels=64,\n",
        "            output_channels=128,\n",
        "            downsample=(2,2),\n",
        "            embedding_size=512,\n",
        "            momentum=0.01\n",
        "        )\n",
        "\n",
        "        self.EncoderBlock4 = EncoderBlock(\n",
        "            input_channels=128,\n",
        "            output_channels=256,\n",
        "            downsample=(2,2),\n",
        "            embedding_size=512,\n",
        "            momentum=0.01\n",
        "        )\n",
        "\n",
        "        self.EncoderBlock5 = EncoderBlock(\n",
        "            input_channels=256,\n",
        "            output_channels=384,\n",
        "            downsample=(2,2),\n",
        "            embedding_size=512,\n",
        "            momentum=0.01\n",
        "        )\n",
        "\n",
        "        self.EncoderBlock6 = EncoderBlock(\n",
        "            input_channels=384,\n",
        "            output_channels=384,\n",
        "            downsample=(1,2),\n",
        "            embedding_size=512,\n",
        "            momentum=0.01\n",
        "        )\n",
        "\n",
        "        self.EncoderBlock7 = EncoderBlock(\n",
        "            input_channels=384,\n",
        "            output_channels=384,\n",
        "            downsample=(1,1),\n",
        "            momentum=0.01,\n",
        "            embedding_size=512\n",
        "        )\n",
        "\n",
        "        self.DecoderBlock1 = DecoderBlock(\n",
        "            input_size=384,\n",
        "            output_size= 384,\n",
        "            embedding_size= 512,\n",
        "            momentum=0.01,\n",
        "            upsample=(1,2)\n",
        "            )\n",
        "\n",
        "        self.DecoderBlock2 = DecoderBlock(\n",
        "            input_size=384,\n",
        "            output_size= 384,\n",
        "            embedding_size= 512,\n",
        "            momentum=0.01,\n",
        "            upsample=(2,2)\n",
        "            )\n",
        "\n",
        "        self.DecoderBlock3 = DecoderBlock(\n",
        "            input_size=384,\n",
        "            output_size= 256,\n",
        "            embedding_size= 512,\n",
        "            momentum=0.01,\n",
        "            upsample=(2,2)\n",
        "            )\n",
        "\n",
        "        self.DecoderBlock4 = DecoderBlock(\n",
        "            input_size=256,\n",
        "            output_size= 128,\n",
        "            embedding_size= 512,\n",
        "            momentum=0.01,\n",
        "            upsample=(2,2)\n",
        "            )\n",
        "\n",
        "        self.DecoderBlock5 = DecoderBlock(\n",
        "            input_size=128,\n",
        "            output_size= 64,\n",
        "            embedding_size= 512,\n",
        "            momentum=0.01,\n",
        "            upsample=(2,2)\n",
        "            )\n",
        "\n",
        "        self.DecoderBlock6 = DecoderBlock(\n",
        "            input_size=64,\n",
        "            output_size= 32,\n",
        "            embedding_size= 512,\n",
        "            momentum=0.01,\n",
        "            upsample=(2,2)\n",
        "            )\n",
        "\n",
        "\n",
        "        self.after_conv = nn.Conv2d(\n",
        "            in_channels=32,\n",
        "            out_channels=2*3,\n",
        "            kernel_size=(1, 1),\n",
        "            stride=(1, 1),\n",
        "            padding=(0, 0),\n",
        "            bias=True,\n",
        "        )\n",
        "\n",
        "        self.init_weights()\n",
        "    def init_weights(self):\n",
        "        init_bn(self.batch_norm0)\n",
        "        init_layer(self.preconvolution)\n",
        "        init_layer(self.after_conv)\n",
        "\n",
        "    def forward(self,input,embedding_vector):\n",
        "        #x -->mag\n",
        "        #coss--->coss\n",
        "        #sins--->sins\n",
        "        audio_length = input.shape[-1]\n",
        "        mags,sins,coss = from_audio_to_spectogram(input)\n",
        "        x = mags\n",
        "        x = x.transpose(1,3)\n",
        "        x = self.batch_norm0(x)\n",
        "        x = x.transpose(1,3)\n",
        "        origin_len = x.shape[2]\n",
        "        pad_len = (int(np.ceil(x.shape[2] / 2**5)) * (2**5)- origin_len)\n",
        "        x = F.pad(x, pad=(0, 0, 0, pad_len))\n",
        "        x = x[:,:,:,0:-1]\n",
        "        x = self.preconvolution(x)\n",
        "\n",
        "        x1, x1_pool = self.EncoderBlock1(x,embedding_vector)\n",
        "        x2, x2_pool = self.EncoderBlock2(x1_pool,embedding_vector)\n",
        "        x3, x3_pool = self.EncoderBlock3(x2_pool,embedding_vector)\n",
        "        x4, x4_pool = self.EncoderBlock4(x3_pool,embedding_vector)\n",
        "        x5, x5_pool = self.EncoderBlock5(x4_pool,embedding_vector)\n",
        "        x6, x6_pool = self.EncoderBlock6(x5_pool,embedding_vector)\n",
        "        x_c,x_c_pool = self.EncoderBlock7(x6_pool,embedding_vector)\n",
        "        x7 = self.DecoderBlock1(x_c,x6,embedding_vector)\n",
        "        x8 = self.DecoderBlock2(x7,x5,embedding_vector)\n",
        "        x9 = self.DecoderBlock3(x8,x4,embedding_vector)\n",
        "        x10 = self.DecoderBlock4(x9,x3,embedding_vector)\n",
        "        x11 = self.DecoderBlock5(x10,x2,embedding_vector)\n",
        "        x12 = self.DecoderBlock6(x11,x1,embedding_vector)\n",
        "        x = self.after_conv(x12)\n",
        "        x = F.pad(x, pad=(0, 1))\n",
        "        x = x[:, :, 0:origin_len, :]\n",
        "        batch_size,_,time_steps,frequency_bins = x.shape\n",
        "        x = x.reshape(\n",
        "            batch_size,   #batch size\n",
        "            1,   #target audio\n",
        "            2,   #num channels\n",
        "            3,   #magnitude, cos, sin\n",
        "            time_steps, #time_steps\n",
        "            frequency_bins, #frequency bins\n",
        "        )\n",
        "\n",
        "        mask_mag = torch.sigmoid(x[:, :, :, 0, :, :]) # get the magnitude mask\n",
        "        _mask_real = torch.tanh(x[:, :, :, 1, :, :])  # get the real component mask\n",
        "        _mask_imag = torch.tanh(x[:, :, :, 2, :, :])  # get the imaginary component mask\n",
        "        _, mask_cos, mask_sin = magphase(_mask_real, _mask_imag)  # get the fourier transform of the mask\n",
        "\n",
        "        # apply the filtering functions to the cosine\n",
        "\n",
        "        out_cos = (\n",
        "            coss[:, None, :, :, :] * mask_cos - sins[:, None, :, :, :] * mask_sin\n",
        "        )\n",
        "\n",
        "        # apply the filtering function to the sine\n",
        "\n",
        "        out_sin = (\n",
        "            sins[:, None, :, :, :] * mask_cos + coss[:, None, :, :, :] * mask_sin\n",
        "        )\n",
        "\n",
        "        #apply the filtering function to the magnitude\n",
        "\n",
        "        out_mag = F.relu_(mags[:, None, :, :, :] * mask_mag)\n",
        "\n",
        "        out_real = out_mag * out_cos\n",
        "        out_imag = out_mag * out_sin\n",
        "\n",
        "        out_real = out_real.reshape(2*batch_size,1,time_steps,frequency_bins)\n",
        "        out_imag = out_imag.reshape(2*batch_size,1,time_steps,frequency_bins)\n",
        "        x = istft(out_real, out_imag, audio_length)\n",
        "        waveform = x.reshape(batch_size,2,audio_length)\n",
        "\n",
        "        return waveform"
      ],
      "metadata": {
        "id": "ZdN-INXbzS8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rete = ResUnet(2,2).to(device='cuda')"
      ],
      "metadata": {
        "id": "A3vH_Bz49Npy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_steps = 10000\n",
        "batch_size = 5\n",
        "\n",
        "for i in range(training_steps):\n",
        "  batch,target,labels,isAudio = get_batch(batch_size,\"./download\",\"./Neural-Networks-Mastrandrea-Frangella/new_balanced.csv\",\"text\")\n",
        "  audio_embeddings = model.get_text_embedding(labels,use_tensor=True).to(device=\"cuda\")\n",
        "  batch = batch.to(device=\"cuda\")\n",
        "  target = target.to(device=\"cuda\")\n",
        "  output = rete(batch,audio_embeddings)\n",
        "  loss = rete.loss(output,target)\n",
        "  print(\"iteration\",i,loss)\n",
        "  # Backward and optimize\n",
        "  rete.optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  rete.optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dIvw6YsZBHYJ",
        "outputId": "c1231fd5-d5fc-4967-8a2a-649b7771c961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0 tensor(0.0953, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 1 tensor(0.1236, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 2 tensor(0.0879, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 3 tensor(0.0540, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 4 tensor(0.0841, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 5 tensor(0.0999, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 6 tensor(0.1044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 7 tensor(0.0230, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 8 tensor(0.0778, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 9 tensor(0.0791, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 10 tensor(0.2374, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 11 tensor(0.0779, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 12 tensor(0.0902, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 13 tensor(0.0507, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 14 tensor(0.1409, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 15 tensor(0.0443, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 16 tensor(0.1543, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 17 tensor(0.0871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 18 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 19 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 20 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 21 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 22 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 23 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 24 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 25 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 26 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 27 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 28 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 29 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 30 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 31 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 32 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 33 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 34 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 35 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 36 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 37 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 38 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 39 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 40 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 41 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 42 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 43 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 44 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 45 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 46 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 47 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 48 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 49 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 50 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 51 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 52 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 53 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 54 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 55 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 56 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 57 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 58 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 59 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 60 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 61 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 62 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 63 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 64 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 65 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 66 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 67 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 68 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 69 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 70 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 71 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 72 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 73 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 74 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 75 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 76 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 77 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 78 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 79 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 80 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 81 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 82 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 83 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 84 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 85 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 86 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 87 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 88 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 89 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 90 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 91 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 92 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 93 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 94 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 95 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 96 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 97 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 98 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 99 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 100 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 101 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 102 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 103 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 104 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 105 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 106 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 107 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 108 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 109 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 110 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 111 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 112 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 113 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 114 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 115 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 116 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 117 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 118 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 119 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 120 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 121 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 122 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 123 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 124 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 125 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 126 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 127 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 128 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 129 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 130 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 131 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 132 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 133 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 134 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 135 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 136 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 137 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 138 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 139 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 140 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 141 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 142 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 143 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 144 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 145 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 146 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 147 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 148 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 149 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 150 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 151 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 152 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 153 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 154 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 155 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 156 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 157 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 158 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 159 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 160 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 161 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 162 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 163 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 164 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 165 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 166 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 167 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 168 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 169 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 170 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 171 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 172 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 173 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 174 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 175 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 176 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 177 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 178 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 179 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 180 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 181 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 182 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 183 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 184 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 185 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 186 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 187 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 188 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 189 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 190 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 191 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 192 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 193 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 194 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 195 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 196 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 197 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 198 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 199 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 200 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 201 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 202 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 203 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 204 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 205 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 206 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 207 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 208 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 209 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 210 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 211 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 212 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 213 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 214 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 215 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 216 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 217 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 218 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 219 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 220 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 221 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 222 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 223 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 224 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 225 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 226 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 227 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 228 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 229 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 230 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 231 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 232 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 233 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 234 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 235 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 236 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 237 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 238 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 239 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 240 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 241 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 242 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 243 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 244 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 245 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 246 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 247 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 248 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 249 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 250 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 251 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 252 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 253 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 254 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 255 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 256 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 257 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 258 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 259 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 260 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 261 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 262 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 263 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 264 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 265 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 266 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 267 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 268 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 269 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 270 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 271 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 272 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 273 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 274 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 275 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 276 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 277 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 278 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 279 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 280 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 281 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 282 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 283 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 284 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 285 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 286 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 287 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 288 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 289 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 290 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 291 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 292 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 293 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 294 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 295 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 296 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 297 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 298 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 299 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 300 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 301 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 302 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 303 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 304 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 305 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 306 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 307 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 308 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 309 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 310 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 311 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 312 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 313 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 314 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 315 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 316 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 317 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 318 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 319 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 320 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 321 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 322 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 323 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 324 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 325 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 326 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 327 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 328 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 329 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 330 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 331 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 332 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 333 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 334 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 335 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 336 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 337 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 338 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 339 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 340 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 341 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 342 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 343 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 344 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 345 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 346 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 347 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 348 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 349 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 350 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 351 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 352 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 353 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 354 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 355 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 356 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 357 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 358 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 359 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 360 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 361 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 362 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 363 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 364 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 365 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 366 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 367 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 368 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 369 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 370 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 371 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 372 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 373 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 374 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 375 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 376 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 377 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 378 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 379 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 380 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 381 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 382 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 383 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 384 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 385 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 386 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 387 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 388 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 389 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 390 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 391 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 392 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 393 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 394 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "iteration 395 tensor(nan, device='cuda:0', grad_fn=<MeanBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-8f309281b437>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;31m# Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mrete\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mrete\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = output.to(device=\"cpu\")\n",
        "target = target.to(device=\"cpu\")\n",
        "batch = batch.to(device=\"cpu\")\n",
        "print(labels[0])\n",
        "torchaudio.save(\"./mixed.mp3\",batch[0],32000)\n",
        "#torchaudio.save(\"./res.mp3\",res[0],32000)\n",
        "#torchaudio.save(\"./target.mp3\",target[0],32000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6iGYz-KB0mp",
        "outputId": "3df716ea-234c-4ffa-f1af-3679aeda7d23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zither Pizzicato\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o26RtQbbJvcs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}